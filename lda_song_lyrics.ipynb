{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Song Lyrics Analysis\n",
    "\n",
    "This notebook performs a multi-faceted analysis on song lyrics:\n",
    "1.  **Unsupervised Topic Modeling**: LDA, LSA, NMF (Discovering hidden topics).\n",
    "2.  **Guided Topic Analysis**: Dictionary-based classification (Tracking specific topics like War, Love).\n",
    "3.  **Temporal Analysis**: How topics evolve over decades.\n",
    "4.  **Sentiment Analysis**: Emotional tone of topics.\n",
    "5.  **Visualization**: Word Clouds and Interactive Maps.\n",
    "\n",
    "**Note**: This notebook is optimized to handle large datasets (9GB+) by using sampling for unsupervised learning and chunking for guided analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# Download NLTK resources\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('sentiment/vader_lexicon.zip')\n",
    "except LookupError:\n",
    "    nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize Spacy\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "except OSError:\n",
    "    print(\"Spacy model 'en_core_web_sm' not found. Please run: python -m spacy download en_core_web_sm\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration & Data Loading (Sample for Unsupervised Models)\n",
    "\n",
    "For **LDA/LSA/NMF**, I use a sample. Training these models on 5 million documents is extremely slow and memory-intensive. \n",
    "A sample of **500,000** songs is statistically very robust and fits in 32GB RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "DATA_PATH = 'song_lyrics.csv'\n",
    "\n",
    "# Recommended for 32GB RAM: 500,000. \n",
    "# If you want a quick test, use 10,000.\n",
    "SAMPLE_SIZE = 500000 \n",
    "\n",
    "print(f\"Loading sample of {SAMPLE_SIZE} rows for Unsupervised Learning (LDA/LSA/NMF)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Reading only necessary columns to save memory\n",
    "    df = pd.read_csv(DATA_PATH, usecols=['lyrics', 'language', 'title', 'artist', 'year'])\n",
    "except ValueError:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"File loaded in {time.time() - start_time:.2f} seconds. Filtering data...\")\n",
    "\n",
    "# Filter for English\n",
    "if 'language' in df.columns:\n",
    "    df = df[df['language'] == 'en']\n",
    "\n",
    "# Drop missing\n",
    "df = df.dropna(subset=['lyrics', 'year'])\n",
    "\n",
    "# Clean Year\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "df = df.dropna(subset=['year'])\n",
    "df['year'] = df['year'].astype(int)\n",
    "df = df[(df['year'] >= 1950) & (df['year'] <= 2024)]\n",
    "\n",
    "# Sample\n",
    "if SAMPLE_SIZE and len(df) > SAMPLE_SIZE:\n",
    "    df_sample = df.sample(SAMPLE_SIZE, random_state=42)\n",
    "    print(f\"Created sample of {len(df_sample)} rows.\")\n",
    "else:\n",
    "    df_sample = df\n",
    "    print(f\"Using full filtered dataset: {len(df_sample)} rows.\")\n",
    "\n",
    "df_sample['decade'] = (df_sample['year'] // 10) * 10\n",
    "data_sample = df_sample['lyrics'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Functions\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "lyric_stopwords = [\n",
    "    'song', 'lyrics', 'chorus', 'verse', 'hook', 'bridge', 'intro', 'outro', 'repeat', \n",
    "    'la', 'da', 'na', 'ooh', 'ahh', 'oh', 'yeah', 'hey', 'whoa', 'hmm', 'baby', 'mh', 'uh', 'huh', 'wow', 'boom', 'bang', 'doo', 'ah', 'ha',\n",
    "    'wanna', 'gonna', 'gotta', 'cause', 'em', 'till', 'ain', 'bout', \n",
    "    'like', 'know', 'got', 'get', 'go', 'come', 'say', 'tell', 'think', 'see', 'look', 'take', 'make', 'want', 'need', 'feel',\n",
    "    'way', 'time', 'thing', 'man', 'girl', 'boy', 'woman', 'day', 'night', 'life', 'world', 'heart', 'love', \n",
    "    'let', 'put', 'back', 'right', 'never', 'ever', 'one', 'two', 'three', 'yeah', 'uh', 'yo'\n",
    "]\n",
    "keywords_to_keep = ['love', 'life', 'world', 'heart', 'war', 'god', 'money', 'fight', 'die', 'party']\n",
    "lyric_stopwords = [w for w in lyric_stopwords if w not in keywords_to_keep]\n",
    "stop_words.extend(lyric_stopwords)\n",
    "\n",
    "def preprocess_data(data):\n",
    "    total = len(data)\n",
    "    print(f\"Starting preprocessing for {total} documents...\")\n",
    "    \n",
    "    print(\"1/4 Tokenizing...\")\n",
    "    data_words = list(sent_to_words(data))\n",
    "    \n",
    "    print(\"2/4 Building Bigrams...\")\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=50)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    \n",
    "    print(\"3/4 Removing Stopwords & Forming Bigrams...\")\n",
    "    data_words_nostops = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in data_words]\n",
    "    data_words_bigrams = [bigram_mod[doc] for doc in data_words_nostops]\n",
    "    \n",
    "    print(\"4/4 Lemmatizing (this is the slowest part)...\")\n",
    "    data_lemmatized = []\n",
    "    for i, sent in enumerate(data_words_bigrams):\n",
    "        if i % 5000 == 0:\n",
    "            print(f\"   Processed {i}/{total} documents...\", end='\\r')\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        data_lemmatized.append([token.lemma_ for token in doc if token.pos_ in ['NOUN', 'PROPN', 'ADJ']])\n",
    "    print(f\"\\nPreprocessing complete.\")\n",
    "    return data_lemmatized\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "start_time = time.time()\n",
    "data_lemmatized = preprocess_data(data_sample)\n",
    "print(f\"Total preprocessing time: {time.time() - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dictionary and Corpus\n",
    "\n",
    "Shared dictionary and corpus for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Dictionary and Corpus...\")\n",
    "id2word = gensim.corpora.Dictionary(data_lemmatized)\n",
    "id2word.filter_extremes(no_below=10, no_above=0.4)\n",
    "corpus = [id2word.doc2bow(text) for text in data_lemmatized]\n",
    "\n",
    "print(f\"Unique tokens: {len(id2word)}\")\n",
    "print(f\"Documents: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training & Comparison (Unsupervised)\n",
    "\n",
    "Training LDA, LSA, and NMF to discover hidden topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Enable logging for Gensim to show progress\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "num_topics = 10\n",
    "models = {}\n",
    "coherence_scores = {}\n",
    "\n",
    "# --- 1. LDA ---\n",
    "print(\"Training LDA Model (this may take a few minutes)...\")\n",
    "print(\"Check the logs below for per-pass progress.\")\n",
    "start_time = time.time()\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics, random_state=100, passes=10)\n",
    "models['LDA'] = lda_model\n",
    "print(f\"LDA Training finished in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 2. LSA (LSI) ---\n",
    "print(\"Training LSA Model...\")\n",
    "lsa_model = gensim.models.LsiModel(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "models['LSA'] = lsa_model\n",
    "\n",
    "# --- 3. NMF ---\n",
    "print(\"Training NMF Model...\")\n",
    "nmf_model = gensim.models.Nmf(corpus=corpus, id2word=id2word, num_topics=num_topics, random_state=100)\n",
    "models['NMF'] = nmf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation: Coherence Scores\n",
    "\n",
    "Comparing the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating Coherence Scores...\")\n",
    "for name, model in models.items():\n",
    "    cm = CoherenceModel(model=model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_scores[name] = cm.get_coherence()\n",
    "    print(f\"{name} Coherence Score: {coherence_scores[name]:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(coherence_scores.keys(), coherence_scores.values(), color=['blue', 'orange', 'green'])\n",
    "plt.title('Coherence Score Comparison')\n",
    "plt.ylabel('Coherence Score (c_v)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Analysis (LDA)\n",
    "\n",
    "I will now use the LDA model to see how topics change over decades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get Topic Distribution for each document\n",
    "print(\"Inferring topics for all documents...\")\n",
    "\n",
    "# Get dominant topic and probability for each doc\n",
    "topic_data = []\n",
    "for i, row in enumerate(lda_model[corpus]):\n",
    "    row = sorted(row, key=lambda x: x[1], reverse=True)\n",
    "    dominant_topic = row[0][0]\n",
    "    dominant_prob = row[0][1]\n",
    "    topic_data.append([dominant_topic, dominant_prob])\n",
    "\n",
    "df_topics = pd.DataFrame(topic_data, columns=['Dominant_Topic', 'Topic_Prob'])\n",
    "df_reset = df_sample.reset_index(drop=True)\n",
    "df_final = pd.concat([df_reset, df_topics], axis=1)\n",
    "\n",
    "# 2. Analyze Topic Trends by Decade\n",
    "decade_topic_counts = df_final.groupby(['decade', 'Dominant_Topic']).size().reset_index(name='Count')\n",
    "decade_counts = df_final.groupby('decade').size().reset_index(name='Total_Songs')\n",
    "decade_topic_dist = pd.merge(decade_topic_counts, decade_counts, on='decade')\n",
    "decade_topic_dist['Percentage'] = decade_topic_dist['Count'] / decade_topic_dist['Total_Songs']\n",
    "\n",
    "# Get Topic Keywords\n",
    "topic_labels = {}\n",
    "for i in range(num_topics):\n",
    "    words = lda_model.show_topic(i, topn=3)\n",
    "    topic_labels[i] = f\"T{i}: \" + \", \".join([w[0] for w in words])\n",
    "\n",
    "decade_topic_dist['Topic_Label'] = decade_topic_dist['Dominant_Topic'].map(topic_labels)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(data=decade_topic_dist, x='decade', y='Percentage', hue='Topic_Label', marker='o')\n",
    "plt.title('LDA Topic Trends Over Decades')\n",
    "plt.ylabel('Percentage of Songs')\n",
    "plt.xlabel('Decade')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sentiment Analysis by Topic\n",
    "\n",
    "I use VADER to calculate the sentiment of each song and then aggregate by topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating sentiment scores (this may take a while)...\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_with_progress(text, i, total):\n",
    "    if i % 5000 == 0:\n",
    "        print(f\"   Processed {i}/{total} songs...\", end='\\r')\n",
    "    return sia.polarity_scores(str(text))['compound']\n",
    "\n",
    "total_rows = len(df_final)\n",
    "df_final['Sentiment'] = [get_sentiment_with_progress(txt, i, total_rows) for i, txt in enumerate(df_final['lyrics'])]\n",
    "print(\"\\nSentiment calculation complete.\")\n",
    "\n",
    "# Group by Topic and calculate mean sentiment\n",
    "topic_sentiment = df_final.groupby('Dominant_Topic')['Sentiment'].mean().reset_index()\n",
    "topic_sentiment['Topic_Label'] = topic_sentiment['Dominant_Topic'].map(topic_labels)\n",
    "\n",
    "# Plot Average Sentiment\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=topic_sentiment, x='Topic_Label', y='Sentiment', palette='coolwarm')\n",
    "plt.title('Average Sentiment by LDA Topic')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Compound Sentiment Score (-1 to 1)')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Word Clouds\n",
    "\n",
    "Visualizing the top words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "cols = 3\n",
    "rows = int(np.ceil(num_topics / cols))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "gs = gridspec.GridSpec(rows, cols)\n",
    "\n",
    "for i in range(num_topics):\n",
    "    plt.subplot(gs[i])\n",
    "    topic_words = dict(lda_model.show_topic(i, 30))\n",
    "    wc = WordCloud(background_color='white', max_words=30)\n",
    "    wc.generate_from_frequencies(topic_words)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Topic {i}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Visualization (pyLDAvis)\n",
    "\n",
    "Interactive exploration of topics and their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Guided Topic Analysis (FULL DATASET - CHUNKING)\n",
    "\n",
    "Here I analyze the **ENTIRE** dataset by reading it in chunks. This allows me to classify millions of songs without running out of RAM.\n",
    "I define fixed topics and count how many songs fall into each category per decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Fixed Topics\n",
    "fixed_topics = {\n",
    "    'Love/Romance': ['love', 'heart', 'baby', 'kiss', 'miss', 'hold', 'feel', 'darling', 'honey'],\n",
    "    'War/Violence': ['war', 'fight', 'gun', 'kill', 'soldier', 'blood', 'army', 'battle', 'bomb', 'death', 'shoot'],\n",
    "    'Party/Fun': ['party', 'dance', 'drink', 'night', 'fun', 'club', 'music', 'dj', 'celebrate', 'rock'],\n",
    "    'Life/Existential': ['life', 'world', 'time', 'live', 'die', 'god', 'soul', 'mind', 'dream', 'truth'],\n",
    "    'Money/Success': ['money', 'rich', 'cash', 'gold', 'fame', 'dollar', 'pay', 'diamond', 'star']\n",
    "}\n",
    "\n",
    "def classify_text_simple(text, topics):\n",
    "    if not isinstance(text, str):\n",
    "        return 'Other'\n",
    "    text = text.lower()\n",
    "    scores = {topic: 0 for topic in topics}\n",
    "    for topic, keywords in topics.items():\n",
    "        for word in keywords:\n",
    "            if word in text:\n",
    "                scores[topic] += 1\n",
    "    max_topic = max(scores, key=scores.get)\n",
    "    if scores[max_topic] == 0:\n",
    "        return 'Other'\n",
    "    return max_topic\n",
    "\n",
    "# Initialize Counters\n",
    "decade_topic_counts = Counter()\n",
    "decade_total_counts = Counter()\n",
    "\n",
    "CHUNK_SIZE = 100000\n",
    "print(\"Starting Full Dataset Analysis (Chunked)...\")\n",
    "\n",
    "# Read CSV in chunks\n",
    "try:\n",
    "    chunk_iter = pd.read_csv(DATA_PATH, usecols=['lyrics', 'language', 'year'], chunksize=CHUNK_SIZE)\n",
    "except ValueError:\n",
    "    # Fallback if columns differ\n",
    "    chunk_iter = pd.read_csv(DATA_PATH, chunksize=CHUNK_SIZE)\n",
    "\n",
    "total_processed = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for i, chunk in enumerate(chunk_iter):\n",
    "    # Filter English\n",
    "    if 'language' in chunk.columns:\n",
    "        chunk = chunk[chunk['language'] == 'en'].copy()\n",
    "    else:\n",
    "        chunk = chunk.copy()\n",
    "    \n",
    "    # Clean Year\n",
    "    chunk['year'] = pd.to_numeric(chunk['year'], errors='coerce')\n",
    "    chunk = chunk.dropna(subset=['year', 'lyrics'])\n",
    "    chunk['year'] = chunk['year'].astype(int)\n",
    "    chunk = chunk[(chunk['year'] >= 1950) & (chunk['year'] <= 2024)]\n",
    "    \n",
    "    # Create Decade\n",
    "    chunk['decade'] = (chunk['year'] // 10) * 10\n",
    "    \n",
    "    # Classify\n",
    "    chunk['Guided_Topic'] = chunk['lyrics'].apply(lambda x: classify_text_simple(x, fixed_topics))\n",
    "    \n",
    "    # Update Counts\n",
    "    for _, row in chunk.iterrows():\n",
    "        decade = row['decade']\n",
    "        topic = row['Guided_Topic']\n",
    "        decade_total_counts[decade] += 1\n",
    "        if topic != 'Other':\n",
    "            decade_topic_counts[(decade, topic)] += 1\n",
    "            \n",
    "    total_processed += len(chunk)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Processed chunk {i+1} (Total rows: {total_processed}). Elapsed: {elapsed:.0f}s\", end='\\r')\n",
    "\n",
    "print(f\"\\nFull Dataset Analysis Complete. Processed {total_processed} rows in {elapsed:.0f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Guided Trends (Full Data)\n",
    "\n",
    "# Convert counters to DataFrame\n",
    "plot_data = []\n",
    "for (decade, topic), count in decade_topic_counts.items():\n",
    "    total = decade_total_counts[decade]\n",
    "    if total > 0:\n",
    "        percentage = count / total\n",
    "        plot_data.append({'decade': decade, 'Guided_Topic': topic, 'Percentage': percentage})\n",
    "\n",
    "df_plot = pd.DataFrame(plot_data).sort_values('decade')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.lineplot(data=df_plot, x='decade', y='Percentage', hue='Guided_Topic', marker='o', linewidth=2.5)\n",
    "plt.title('Guided Topic Trends Over Decades (FULL DATASET)')\n",
    "plt.ylabel('Percentage of Songs')\n",
    "plt.xlabel('Decade')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
